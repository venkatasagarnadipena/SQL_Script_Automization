Overview
This script automates the generation of SQL CREATE TABLE commands in a Delta Lake environment based on configurations provided either directly in the code or through a CSV file. It allows users to define table structures, including column names, data types, nullability, partitioning, and other properties.

Code Breakdown
Input Definitions (Blocks 1-2):

Initialize lists to hold column names, data types, nullability, target table names, storage paths, and partitioning information.
SQL Command Generation (Blocks 3-5):

Construct the SQL command based on the user inputs, ensuring the lengths of the input lists match.
Build the column definitions string, accounting for whether columns are nullable.
Data Reading from CSV (Block 6):

Load table configurations from a CSV file into a Spark DataFrame, if applicable.
Iterative Command Creation (Blocks 7-8):

Loop through each row of the DataFrame or process the predefined lists to generate and print SQL commands for each table.
Options for Using the Script
Without CSV:

Define the table structure directly in the code by populating the lists:
column_name: List of column names.
table_schema: List of data types.
nullable: List of nullability indicators.
target_table_name: Name of the target table.
path: Path for data storage.
partition_column: List of partition columns (if any).
Execute the script, and it will generate the SQL commands based on these inputs.
With CSV:

Prepare the CSV File: Create a CSV file (createtable.csv) with the following columns:
table_columns: Comma-separated list of column names.
table_schema: Comma-separated list of data types for each column.
null_factor: Comma-separated list indicating nullable columns (empty, "null", or "true" for nullable).
table_name: Name of the target table.
table_path: Path where the table's data will be stored.
table_partition: Comma-separated list of partition columns (if any).
partition_flag: Indicates if the table should be partitioned ("Y" or "y" for yes).
db_schema: Schema name for the database.
null_factor_flag: Indicates if the nullability factors should be checked ("Y" or "y" for yes).
storage_account and container_name: Details for Azure storage.
Load the Data:

For the CSV approach, ensure the script reads the CSV file using Spark to create a DataFrame.
Run the Script:

Execute the script in a Databricks or Spark environment. The generated SQL commands will be printed out for each table configuration.
Review the Output:

The output will include the SQL commands for creating tables, which can be executed in the relevant database environment.
